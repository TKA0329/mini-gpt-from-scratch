{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTf-bBKdopQ"
      },
      "source": [
        "# train main model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npqC7GNSdn4T",
        "outputId": "c28ba62c-bea5-411a-b3ba-99913718ff95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n",
            "[203, 38, 19934, 823]\n",
            "Hello world\n",
            "tensor([19930,     6,   133,  1299,    32,   167,   310,   819,    84,   112,\n",
            "           28,  3234,    88,  3690,    88,    48,   249,  1621, 19948,    13,\n",
            "         6502,    32,   102,  1597,   142,    48,   127,  1472,   169,    13,\n",
            "          264, 10348,  3317,   246,    48,    93,  2931,   269,  4876,   101,\n",
            "          143,    75,  3603,   190,  4999,  2795, 19948,    31,   142, 19948,\n",
            "         2020,    32,   512, 15714,   190,  1699,  2061, 19948,   371,   209,\n",
            "         5386,  3728,  5133,   154,    13,  4153,    32,  3902,    31,    13,\n",
            "         3436,    32,   554, 19989,  1000,    13,  5497,   142,   794,   151,\n",
            "         1427,    13, 10517,  3603,  3307,   882,    13,   435, 16797,   253,\n",
            "          723,  6631, 10768,  1484,    32,   194,  5389,    28,    13,   346])\n",
            "<|new_book|> The object of this Essay is to explain as clearly as I am able, the grounds of an opinion which I have held from the very earliest period when I had formed any opinions at all on social or political matters, and which, instead of being weakened or modified, has been constantly growing stronger by the progress of reflection and the experience of life: That the principle which regulates the existing social relations between the two sexes--the legal subordination of one sex to the other\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "import math\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "# set up device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "if not os.path.exists(\"unified_tokenizer.model\"):\n",
        "  # sentencepiece to get tokens\n",
        "  spm.SentencePieceTrainer.Train(\n",
        "      input=\"output.txt,chatbot_dataset.txt\",\n",
        "      model_prefix=\"unified_tokenizer\",\n",
        "      vocab_size = 20000,\n",
        "      model_type=\"bpe\",\n",
        "      character_coverage=0.9995, # bcz it contains foreign language\n",
        "      user_defined_symbols=[\"<|user|>\", \"<|bot|>\", \"<EOS>\", \"<|new_book|>\"] # must be a list\n",
        "  )\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"unified_tokenizer.model\")\n",
        "# testing if it works\n",
        "ids = sp.encode(\"Hello world\", out_type=int)\n",
        "print(ids)\n",
        "text = sp.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# loading tokenizer model + testing\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"unified_tokenizer.model\")\n",
        "encode = lambda s: sp.encode(s, out_type=int)\n",
        "# Without out_type=int, sp.encode() might return a list of strings instead of IDs\n",
        "decode = lambda l: sp.decode(l)\n",
        "\n",
        "with open(\"output.txt\", encoding=\"utf-8\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data[:100])\n",
        "print(decode((data[:100]).tolist()))\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nPIg49ac5bmF"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "block_size = 512 # block = sequence length\n",
        "batch_size = 16\n",
        "learning_rate = 3e-4\n",
        "n_embd= 384\n",
        "dropout = 0.1 # 10% of neurons dropped out\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "vocab_size = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yzltXOvn52wG"
      },
      "outputs": [],
      "source": [
        "# Creating a custom dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "  def __init__(self, data, block_size):\n",
        "    self.data = data # tensor with a bunch of tokens\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\" returns the number of samples \"\"\"\n",
        "    return len(self.data) - self.block_size\n",
        "    # so our Dataset doesn’t try to grab a sequence that runs past the end of your tokenized text.\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" returns a particular sample in X, y\"\"\"\n",
        "    X = self.data[idx: idx + self.block_size]\n",
        "    y = self.data[idx + 1: idx+1+self.block_size]\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rml4_3Gl59C0",
        "outputId": "e2794f6a-1387-491f-c96a-f248c43447ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9649133\n",
            "16\n",
            "603039\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "train_dataset = TokenDataset(data = train_data,\n",
        "                             block_size=block_size)\n",
        "\n",
        "test_dataset = TokenDataset(data= test_data,\n",
        "                            block_size=block_size)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers = os.cpu_count(),\n",
        "                              pin_memory=False)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=False,\n",
        "                             num_workers=os.cpu_count(),\n",
        "                             pin_memory=False)\n",
        "\n",
        "print(len(train_data))\n",
        "X, y = next(iter(train_dataloader))\n",
        "print(len(X)) # output = batch_size\n",
        "print(len(train_dataloader)) # output = train_data/batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0iZU9lf5Mq3"
      },
      "outputs": [],
      "source": [
        "# transformer\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.query= nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    scores = (q @ k.transpose(-2,-1))*(k.shape[-1]**-0.5) # getting the shape of head_size for scaling\n",
        "    scores = scores.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # slicing it & causal masking\n",
        "    attention_weights = torch.softmax(scores, dim=-1) # doing it to the last dimension\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    v = self.value(x)\n",
        "    out = attention_weights @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self-attention in parallel\"\"\"\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    # For each head h, Head(head_size) receives x and returns [B, T, S].\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.linearprojections = nn.Linear(head_size*n_head, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # The list comprehension [h(x) for h in self.heads] produces H tensors each [B, T, S].\n",
        "    # out has shape [B, T, H * S] which usually equals [B, T, n_embd].\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    # self.linearprojections(out) is a linear layer that maps [B, T, H*S] -> [B, T, E]\n",
        "    out = self.dropout(self.linearprojections(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
        "                             nn.GELU(),\n",
        "                             nn.Linear(4*n_embd, n_embd), # Final output: still [B, T, n_embd]\n",
        "                             nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//n_head\n",
        "    self.multiheadattention = MultiHeadAttention(n_head, head_size)\n",
        "    self.feedforward = FeedForward(n_embd)\n",
        "    self.layernorm1 = nn.LayerNorm(n_embd)\n",
        "    self.layernorm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = x + self.multiheadattention(self.layernorm1(x))\n",
        "    y = y + self.feedforward(self.layernorm2(y))\n",
        "    return y\n",
        "\n",
        "class MiniGPTModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embd):\n",
        "    super().__init__()\n",
        "    self.token_embedding_layer = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_layer = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.layernorm = nn.LayerNorm(n_embd)\n",
        "    self.linear = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
        "    self.linear.weight = self.token_embedding_layer.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear): # make sure weights init properly\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    tok = self.token_embedding_layer(index)\n",
        "    B, T = index.shape\n",
        "    pos = self.position_embedding_layer(torch.arange(T, device=index.device))\n",
        "    x = tok + pos\n",
        "    x = self.blocks(x)\n",
        "    x = self.layernorm(x)\n",
        "    logits = self.linear(x)\n",
        "    if targets is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "\n",
        "  def top_k_sampling(self, logits, temperature=1.0, k=100):\n",
        "    topk_logits, topk_index = torch.topk(logits, k)\n",
        "    topk_logits = topk_logits/temperature\n",
        "    probs = torch.softmax(topk_logits, dim=-1)\n",
        "    idx = torch.multinomial(probs, num_samples=1)\n",
        "    return topk_index[idx]\n",
        "\n",
        "  def generate(self, index, max_token_number, temperature=1.0, k=100):\n",
        "    for i in range(max_token_number):\n",
        "      index_cond = index[:, -block_size:]\n",
        "      logits, loss = self.forward(index_cond)\n",
        "      logits = logits[:, -1, :] # shape [B, vocab_size]\n",
        "      next_token = self.top_k_sampling(logits[0], temperature=temperature, k=k)\n",
        "      next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
        "      next_token = next_token.view(1,1)\n",
        "      index = torch.cat((index, next_token), dim=1)\n",
        "    return index\n",
        "\n",
        "model_GPT = MiniGPTModel(vocab_size, n_embd).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO4v50u05q1C"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "optimizer = torch.optim.AdamW(model_GPT.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 12\n",
        "\n",
        "total_steps = epochs * (len(train_dataloader))   # total training steps\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "# because looping through each batch every epoch\n",
        "print(f\"Total_steps: {total_steps}\")\n",
        "\n",
        "def lr_lambda(current_step):\n",
        "  if current_step < warmup_steps:\n",
        "      # Linear warm-up\n",
        "      return float(current_step) / float(max(1, warmup_steps))\n",
        "  else:\n",
        "    # After warm-up, cosine decay\n",
        "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "    # Adding 1 shifts that range from [1, -1] to [2, 0].\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "results = {\"train_loss\": [],\n",
        "           \"test_loss\": []}\n",
        "norm = []\n",
        "lrs = []\n",
        "best_val_loss = float('inf')\n",
        "global_step=0.0\n",
        "num_steps_per_epoch = len(train_dataloader)\n",
        "log_every = max(1, num_steps_per_epoch //10) # avoid division by zero\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  train_loss = 0.0\n",
        "  model_GPT.train() # sampling randomly at different points\n",
        "  for x, y in train_dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Evaluate the loss\n",
        "    logits, loss = model_GPT(x, y)\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # NaN checks\n",
        "    total_norm = 0.0\n",
        "    for name, param in model_GPT.named_parameters():\n",
        "      if param.grad is not None:\n",
        "        if torch.isnan(param).any():\n",
        "          print(f\"ALERT!! NaN detected in parameters: {name}\")\n",
        "        if torch.isnan(param.grad).any():\n",
        "          print(f\"ALERT!! NaN detected in gradients: {name}\")\n",
        "\n",
        "\n",
        "    # gradient clipping\n",
        "    grad_norm = torch.nn.utils.clip_grad_norm_(model_GPT.parameters(), max_norm = 3.0)\n",
        "    norm.append(grad_norm.item()) # logging it\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    lrs.append(scheduler.get_last_lr()[0])\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step % log_every == 0:\n",
        "      print(f\"-------------------------------------------------\")\n",
        "      print(f\"{epoch} \\n {(global_step/num_steps_per_epoch)*100} % \")\n",
        "      print(f\"LR at {global_step} steps: {scheduler.get_last_lr()[0]}\")\n",
        "      print(f\"Gradient norm: {(grad_norm.item()):.4f}\")\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "  results[\"train_loss\"].append(train_loss.item())\n",
        "\n",
        "  model_GPT.eval()\n",
        "  with torch.inference_mode():\n",
        "    total_val_loss = 0.0\n",
        "    for x_val, y_val in test_dataloader:\n",
        "      x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "      val_logits, val_loss = model_GPT(x_val, y_val)\n",
        "      total_val_loss += val_loss.item()\n",
        "    total_val_loss /= len(test_dataloader)\n",
        "    results[\"test_loss\"].append(total_val_loss)\n",
        "    if epochs % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Train loss: {train_loss} | Val loss: {total_val_loss}\")\n",
        "      # sampling\n",
        "      list_of_words = random.choice([\"He looked at her and said\", \"My dear sir\",\n",
        "                                    \"It was on a dreary night of November.\",\n",
        "                                    \"The candle flickered, casting long shadows.\",\n",
        "                                    \"The street was silent, save for the sound of rain\",\n",
        "                                    \"I stood at the gates, uncertain\",\n",
        "                                    \"Snow fell in thick, silent sheets over the empty street.\",\n",
        "                                    \"Her breath turned to mist in the frigid air as she waited.\",\n",
        "                                    \"But you promised!\",\n",
        "                                    \"Snowflakes melted as they kissed her skin.\",\n",
        "                                    \"The world was white and still, muffled beneath the snow.\",\n",
        "                                    \"The clock struck thirteen\",\n",
        "                                    \"Her hands trembled, but her eyes were steady.\",\n",
        "                                    \"They told me never to open that door, but…\",\n",
        "                                    \"Hello! How are you doing?\"])\n",
        "      context = torch.tensor([encode(list_of_words)], dtype=torch.long).to(device)\n",
        "      print(f\"-------------------------------------------------\")\n",
        "      generated_chars = model_GPT.generate(context.to(device), max_token_number=500, temperature=0.8, k=50)\n",
        "      decoded_chars = decode((generated_chars[0]).tolist())\n",
        "      print(decoded_chars)\n",
        "\n",
        "    if total_val_loss.item() < best_val_loss:\n",
        "      best_val_loss = total_val_loss.item()\n",
        "      torch.save(model_GPT.state_dict(), \"best_model.pth\")\n",
        "      print(\"✅ Saved new best model\")\n",
        "      print(f\"Val loss at this stage: {val_loss.item()}\")\n",
        "\n",
        "    #scheduler.step(val_loss)\n",
        "\n",
        "end_time=timer()\n",
        "print(f\"Total time taken = {end_time-start_time}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkcaAU-t5slP"
      },
      "outputs": [],
      "source": [
        "train_loss = results[\"train_loss\"]\n",
        "test_loss = results[\"test_loss\"]\n",
        "epochs = range((len(results[\"train_loss\"])))\n",
        "\n",
        "plt.figure(figsize=(10,15))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(epochs, train_loss, label=\"train_loss\")\n",
        "plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "epochs = range(len(lrs))\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(epochs, lrs, label=\"Learning rate\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "steps = range(len(norm))\n",
        "plt.plot(steps, norm, label=\"Gradient norm\")\n",
        "plt.xlabel(\"steps\")\n",
        "plt.ylabel(\"Gradient norm\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvFVlpKw5uOW"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "with torch.inference_mode():\n",
        "  prompt = input(\"Prompt: \")\n",
        "  context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "  generated_chars = model_GPT.generate(context.to(device), max_token_number=1000, temperature=0.7, k=50)\n",
        "  # print(f\"Generated characters: {generated_chars}\")\n",
        "  decoded_chars = decode((generated_chars[0]).tolist())\n",
        "  print(decoded_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbl8P1RXE1KI"
      },
      "source": [
        "#loading model for testing on gradio (no chatbot yet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbrwwqG_EiAs"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return f\"Hello {name}!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoFDQ5LhEqzB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "import math\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "\n",
        "MODEL_SAVE_PATH = \"V0_best_model.pth\" # change to appropriate name\n",
        "\n",
        "# set up device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"unified_tokenizer.model\")\n",
        "ids = sp.encode(\"Hello world\", out_type=int)\n",
        "print(ids)\n",
        "text = sp.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 512 # block = sequence length\n",
        "batch_size = 16\n",
        "learning_rate = 3e-4\n",
        "n_embd= 384\n",
        "dropout = 0.1 # 10% of neurons dropped out\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "vocab_size = 20000\n",
        "\n",
        "# loading tokenizer model + testing\n",
        "sp = spm.SentencePieceProcessor()\n",
        "tokenizer = sp.load(\"unified_tokenizer.model\")\n",
        "encode = lambda s: sp.encode(s, out_type=int)\n",
        "# Without out_type=int, sp.encode() might return a list of strings instead of IDs\n",
        "decode = lambda l: sp.decode(l)\n",
        "\n",
        "# transformer\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.query= nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    scores = (q @ k.transpose(-2,-1))*(k.shape[-1]**-0.5) # getting the shape of head_size for scaling\n",
        "    scores = scores.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # slicing it & causal masking\n",
        "    attention_weights = torch.softmax(scores, dim=-1) # doing it to the last dimension\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    v = self.value(x)\n",
        "    out = attention_weights @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self-attention in parallel\"\"\"\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    # For each head h, Head(head_size) receives x and returns [B, T, S].\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.linearprojections = nn.Linear(head_size*n_head, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # The list comprehension [h(x) for h in self.heads] produces H tensors each [B, T, S].\n",
        "    # out has shape [B, T, H * S] which usually equals [B, T, n_embd].\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    # self.linearprojections(out) is a linear layer that maps [B, T, H*S] -> [B, T, E]\n",
        "    out = self.dropout(self.linearprojections(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
        "                             nn.GELU(),\n",
        "                             nn.Linear(4*n_embd, n_embd), # Final output: still [B, T, n_embd]\n",
        "                             nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//n_head\n",
        "    self.multiheadattention = MultiHeadAttention(n_head, head_size)\n",
        "    self.feedforward = FeedForward(n_embd)\n",
        "    self.layernorm1 = nn.LayerNorm(n_embd)\n",
        "    self.layernorm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = x + self.multiheadattention(self.layernorm1(x))\n",
        "    y = y + self.feedforward(self.layernorm2(y))\n",
        "    return y\n",
        "\n",
        "class MiniGPTModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embd):\n",
        "    super().__init__()\n",
        "    self.token_embedding_layer = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_layer = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.layernorm = nn.LayerNorm(n_embd)\n",
        "    self.linear = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
        "    self.linear.weight = self.token_embedding_layer.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear): # make sure weights init properly\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    tok = self.token_embedding_layer(index)\n",
        "    B, T = index.shape\n",
        "    pos = self.position_embedding_layer(torch.arange(T, device=index.device))\n",
        "    x = tok + pos\n",
        "    x = self.blocks(x)\n",
        "    x = self.layernorm(x)\n",
        "    logits = self.linear(x)\n",
        "    if targets is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "\n",
        "  def top_k_sampling(self, logits, temperature=1.0, k=100):\n",
        "    topk_logits, topk_index = torch.topk(logits, k)\n",
        "    topk_logits = topk_logits/temperature\n",
        "    probs = torch.softmax(topk_logits, dim=-1)\n",
        "    idx = torch.multinomial(probs, num_samples=1)\n",
        "    return topk_index[idx]\n",
        "\n",
        "  def generate(self, index, max_token_number=512, temperature=1.0, k=100):\n",
        "    for i in range(max_token_number):\n",
        "      index_cond = index[:, -block_size:]\n",
        "      logits, loss = self.forward(index_cond)\n",
        "      logits = logits[:, -1, :] # shape [B, vocab_size]\n",
        "      next_token = self.top_k_sampling(logits[0], temperature=temperature, k=k)\n",
        "      next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
        "      next_token = next_token.view(1,1)\n",
        "      index = torch.cat((index, next_token), dim=1)\n",
        "    return index\n",
        "\n",
        "loaded_model = MiniGPTModel(vocab_size, n_embd)\n",
        "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "loaded_model.eval()\n",
        "with torch.inference_mode():\n",
        "  prompt = input(\"Prompt: \")\n",
        "  context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "  generated_chars = loaded_model.generate(context.to(device), max_token_number=1000, temperature=0.8, k=50)\n",
        "  # print(f\"Generated characters: {generated_chars}\")\n",
        "  decoded_chars = decode((generated_chars[0]).tolist())\n",
        "  print(decoded_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY2YLOCFEwyV"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "def tokenise(text):\n",
        "  input = encode(text)\n",
        "  return input\n",
        "\n",
        "def detokenise(input):\n",
        "  output = decode(input)\n",
        "  return output\n",
        "\n",
        "def chat_with_model(user_input, history):\n",
        "  loaded_model.eval()\n",
        "  history = history + [(user_input, \"\")]\n",
        "  yield \"\", history, history\n",
        "  with torch.inference_mode():\n",
        "    context = torch.tensor([tokenise(user_input)], dtype=torch.long).to(device)\n",
        "    generated_chars = loaded_model.generate(context.to(device), max_token_number=250, temperature=0.7, k=50)\n",
        "    decoded_chars = detokenise((generated_chars[0]).tolist())\n",
        "  #Updates the chat history by appending a new tuple (user_input, decoded_chars).\n",
        "  history[-1] = (user_input, decoded_chars)\n",
        "  yield \"\", history, history\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "  chatbot = gr.Chatbot() # visual chat window\n",
        "  msg = gr.Textbox(label=\"You\") # input box for the user\n",
        "  state = gr.State([])  # store chat history # hidden variable to keep conversation history between submissions\n",
        "  msg.submit(chat_with_model, inputs=[msg, state], outputs=[msg, chatbot, state])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufa4QuL1E3ns"
      },
      "source": [
        "# fine tune with chatbot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRytddi3Ezvl"
      },
      "outputs": [],
      "source": [
        "# set up device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "block_size = 512 # block = sequence length\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "n_embd= 384\n",
        "dropout = 0.1 # 10% of neurons dropped out\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "vocab_size = 20000\n",
        "\n",
        "if not os.path.exists(\"unified_tokenizer.model\"):\n",
        "  # sentencepiece to get tokens\n",
        "  spm.SentencePieceTrainer.Train(\n",
        "      input=\"output.txt, chatbot_dataset.txt\",\n",
        "      model_prefix=\"unified_tokenizer\",\n",
        "      vocab_size = 20000,\n",
        "      model_type=\"bpe\",\n",
        "      character_coverage=0.9995, # bcz it contains foreign language\n",
        "      user_defined_symbols=[\"<|user|>\", \"<|bot|>\", \"<EOS>\", \"<|new_book|>\"] # must be a list\n",
        "  )\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"unified_tokenizer.model\")\n",
        "# testing if it works\n",
        "ids = sp.encode(\"Hello world\", out_type=int)\n",
        "print(ids)\n",
        "text = sp.decode(ids)\n",
        "print(text)\n",
        "\n",
        "# loading tokenizer model + testing\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"unified_tokenizer.model\")\n",
        "encode = lambda s: sp.encode(s, out_type=int)\n",
        "# Without out_type=int, sp.encode() might return a list of strings instead of IDs\n",
        "decode = lambda l: sp.decode(l)\n",
        "\n",
        "with open(\"chatbot_dataset.txt\", encoding=\"utf-8\") as file:\n",
        "  text_chatbot = file.read()\n",
        "\n",
        "data_chatbot = torch.tensor(encode(text_chatbot), dtype = torch.long)\n",
        "\n",
        "#testing if it works\n",
        "print(data_chatbot[:100])\n",
        "print(decode((data_chatbot[:100]).tolist()))\n",
        "\n",
        "# splitting dataset for training + testing\n",
        "n = int(0.8*len(data_chatbot))\n",
        "train_data = data_chatbot[:n]\n",
        "test_data = data_chatbot[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9-KLFuvQE_e"
      },
      "outputs": [],
      "source": [
        "# Creating a custom dataset\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "  def __init__(self, data, block_size):\n",
        "    self.data = data # tensor with a bunch of tokens\n",
        "    self.block_size = block_size\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\" returns the number of samples \"\"\"\n",
        "    return len(self.data) - self.block_size\n",
        "    # so our Dataset doesn’t try to grab a sequence that runs past the end of your tokenized text.\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" returns a particular sample in X, y\"\"\"\n",
        "    X = self.data[idx: idx + self.block_size]\n",
        "    y = self.data[idx + 1: idx+1+self.block_size]\n",
        "    return X, y\n",
        "\n",
        "import os\n",
        "train_dataset = TokenDataset(data = train_data,\n",
        "                             block_size=block_size)\n",
        "\n",
        "test_dataset = TokenDataset(data= test_data,\n",
        "                            block_size=block_size)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers = os.cpu_count(),\n",
        "                              pin_memory=False)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=False,\n",
        "                             num_workers=os.cpu_count(),\n",
        "                             pin_memory=False)\n",
        "\n",
        "print(len(train_data))\n",
        "X, y = next(iter(train_dataloader))\n",
        "print(len(X)) # output = batch_size\n",
        "print(len(train_dataloader)) # output = train_data/batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJj0ouhDHP5h"
      },
      "outputs": [],
      "source": [
        "# transformer\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.query= nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=True)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    scores = (q @ k.transpose(-2,-1))*(k.shape[-1]**-0.5) # getting the shape of head_size for scaling\n",
        "    scores = scores.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # slicing it & causal masking\n",
        "    attention_weights = torch.softmax(scores, dim=-1) # doing it to the last dimension\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    v = self.value(x)\n",
        "    out = attention_weights @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" Multiple heads of self-attention in parallel\"\"\"\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    # For each head h, Head(head_size) receives x and returns [B, T, S].\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.linearprojections = nn.Linear(head_size*n_head, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # The list comprehension [h(x) for h in self.heads] produces H tensors each [B, T, S].\n",
        "    # out has shape [B, T, H * S] which usually equals [B, T, n_embd].\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    # self.linearprojections(out) is a linear layer that maps [B, T, H*S] -> [B, T, E]\n",
        "    out = self.dropout(self.linearprojections(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(nn.Linear(n_embd, 4*n_embd),\n",
        "                             nn.GELU(),\n",
        "                             nn.Linear(4*n_embd, n_embd), # Final output: still [B, T, n_embd]\n",
        "                             nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//n_head\n",
        "    self.multiheadattention = MultiHeadAttention(n_head, head_size)\n",
        "    self.feedforward = FeedForward(n_embd)\n",
        "    self.layernorm1 = nn.LayerNorm(n_embd)\n",
        "    self.layernorm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = x + self.multiheadattention(self.layernorm1(x))\n",
        "    y = y + self.feedforward(self.layernorm2(y))\n",
        "    return y\n",
        "\n",
        "class MiniGPTModel(nn.Module):\n",
        "  def __init__(self, vocab_size, n_embd):\n",
        "    super().__init__()\n",
        "    self.token_embedding_layer = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_layer = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.layernorm = nn.LayerNorm(n_embd)\n",
        "    self.linear = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
        "    self.linear.weight = self.token_embedding_layer.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear): # make sure weights init properly\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    tok = self.token_embedding_layer(index)\n",
        "    B, T = index.shape\n",
        "    pos = self.position_embedding_layer(torch.arange(T, device=index.device))\n",
        "    x = tok + pos\n",
        "    x = self.blocks(x)\n",
        "    x = self.layernorm(x)\n",
        "    logits = self.linear(x)\n",
        "    if targets is None:\n",
        "      return logits, None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "      return logits, loss\n",
        "\n",
        "  def top_k_sampling(self, logits, temperature=1.0, k=100):\n",
        "    topk_logits, topk_index = torch.topk(logits, k)\n",
        "    topk_logits = topk_logits/temperature\n",
        "    probs = torch.softmax(topk_logits, dim=-1)\n",
        "    idx = torch.multinomial(probs, num_samples=1)\n",
        "    return topk_index[idx]\n",
        "\n",
        "  def generate(self, index, max_token_number=512, temperature=1.0, k=100):\n",
        "    for i in range(max_token_number):\n",
        "      index_cond = index[:, -block_size:]\n",
        "      logits, loss = self.forward(index_cond)\n",
        "      logits = logits[:, -1, :] # shape [B, vocab_size]\n",
        "      next_token = self.top_k_sampling(logits[0], temperature=temperature, k=k)\n",
        "      next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
        "      next_token = next_token.view(1,1)\n",
        "      index = torch.cat((index, next_token), dim=1)\n",
        "    return index\n",
        "\n",
        "loaded_model_chatbot = MiniGPTModel(vocab_size, n_embd).to(device)\n",
        "loaded_model_chatbot.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "optimizer = torch.optim.AdamW(loaded_model_chatbot.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "total_steps = epochs * (len(train_dataloader))   # total training steps\n",
        "warmup_steps = int(0.05 * total_steps)\n",
        "# because looping through each batch every epoch\n",
        "print(f\"Total_steps: {total_steps}\")\n",
        "\n",
        "def lr_lambda(current_step):\n",
        "  if current_step < warmup_steps:\n",
        "      # Linear warm-up\n",
        "      return float(current_step) / float(max(1, warmup_steps))\n",
        "  else:\n",
        "    # After warm-up, cosine decay\n",
        "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "    # Adding 1 shifts that range from [1, -1] to [2, 0].\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "results = {\"train_loss\": [],\n",
        "           \"test_loss\": []}\n",
        "norm = []\n",
        "lrs = []\n",
        "best_val_loss = float('inf')\n",
        "global_step=0\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  train_loss = 0\n",
        "  loaded_model_chatbot.train() # sampling randomly at different points\n",
        "  for x, y in train_dataloader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Evaluate the loss\n",
        "    logits, loss = loaded_model_chatbot(x, y)\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # NaN checks\n",
        "    total_norm = 0.0\n",
        "    for name, param in loaded_model_chatbot.named_parameters():\n",
        "      if param.grad is not None:\n",
        "        if torch.isnan(param).any():\n",
        "          print(f\"ALERT!! NaN detected in parameters: {name}\")\n",
        "        if torch.isnan(param.grad).any():\n",
        "          print(f\"ALERT!! NaN detected in gradients: {name}\")\n",
        "        param_norm = param.grad.data.norm(2)\n",
        "        total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "    norm.append(total_norm)\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(loaded_model_chatbot.parameters(), max_norm = 3.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    lrs.append(scheduler.get_last_lr()[0])\n",
        "    global_step += 1\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "  results[\"train_loss\"].append(train_loss.item())\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    print(f\"-------------------------------------------------\")\n",
        "    print(f\"LR at {epoch} epochs: {scheduler.get_last_lr()[0]}\")\n",
        "    print(f\"Gradient norm at {epoch} epochs: {total_norm:.4f}\")\n",
        "\n",
        "  loaded_model_chatbot.eval()\n",
        "  with torch.inference_mode():\n",
        "    total_val_loss = 0\n",
        "    for x_val, y_val in test_dataloader:\n",
        "      x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "      val_logits, val_loss = loaded_model_chatbot(x_val, y_val)\n",
        "      total_val_loss += val_loss.item()\n",
        "    avg_val_loss /= len(test_dataloader)\n",
        "    results[\"test_loss\"].append(avg_val_loss.item()) # append every 100 epochs\n",
        "    if epoch % 5 == 0:\n",
        "      print(f\"Epoch {epoch} | Train loss: {train_loss} | Val loss: {avg_val_loss}\")\n",
        "      # sampling\n",
        "      list_of_words = random.choice([\"He looked at her and said\", \"My dear sir\",\n",
        "                                    \"It was on a dreary night of November.\",\n",
        "                                    \"The candle flickered, casting long shadows.\",\n",
        "                                    \"The street was silent, save for the sound of rain\",\n",
        "                                    \"I stood at the gates, uncertain\",\n",
        "                                    \"Snow fell in thick, silent sheets over the empty street.\",\n",
        "                                    \"Her breath turned to mist in the frigid air as she waited.\",\n",
        "                                    \"But you promised!\",\n",
        "                                    \"Snowflakes melted as they kissed her skin.\",\n",
        "                                    \"The world was white and still, muffled beneath the snow.\",\n",
        "                                    \"The clock struck thirteen\",\n",
        "                                    \"Her hands trembled, but her eyes were steady.\",\n",
        "                                    \"They told me never to open that door, but…\",\n",
        "                                    \"Hello! How are you doing?\"])\n",
        "      context = torch.tensor([encode(list_of_words)], dtype=torch.long).to(device)\n",
        "      print(f\"-------------------------------------------------\")\n",
        "      generated_chars = loaded_model_chatbot.generate(context.to(device), max_token_number=500, temperature=0.8, k=50)\n",
        "      decoded_chars = decode((generated_chars[0]).tolist())\n",
        "      print(decoded_chars)\n",
        "\n",
        "    if avg_val_loss.item() < best_val_loss:\n",
        "      best_val_loss = avg_val_loss.item()\n",
        "      torch.save(loaded_model_chatbot.state_dict(), \"best_model.pth\")\n",
        "      print(\"✅ Saved new best model\")\n",
        "      print(f\"Val loss at this stage: {avg_val_loss.item()}\")\n",
        "\n",
        "    #scheduler.step(val_loss)\n",
        "\n",
        "end_time=timer()\n",
        "print(f\"Total time taken = {end_time-start_time}s\")\n",
        "\n",
        "train_loss = results[\"train_loss\"]\n",
        "test_loss = results[\"test_loss\"]\n",
        "epochs = range((len(results[\"train_loss\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEpbRTYWcw7g"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,15))\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(epochs, train_loss, label=\"train_loss\")\n",
        "plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "epochs = range(len(lrs))\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(epochs, lrs, label=\"Learning rate\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "epochs = range(len(norm))\n",
        "plt.plot(epochs, norm, label=\"Gradient norm\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Gradient norm\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(\"train_&_test_plots.pdf\")\n",
        "\n",
        "# Inference\n",
        "with torch.inference_mode():\n",
        "  prompt = input(\"Prompt: \")\n",
        "  context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n",
        "  generated_chars = loaded_model_chatbot.generate(context.to(device), max_token_number=1000, temperature=0.7, k=50)\n",
        "  # print(f\"Generated characters: {generated_chars}\")\n",
        "  decoded_chars = decode((generated_chars[0]).tolist())\n",
        "  print(decoded_chars)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
